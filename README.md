# MedChain: Bridging the Gap Between LLM Agents and Clinical Practice through Interactive Sequential Benchmarking

<p align="center">
  <img src="./assets/logo.png">
</p>

<!-- <i>The avatar is generated by DALLE-3.</i> -->

[ü§ñ **Homepage**](https://cuhk-aim-group.github.io/EndoBench.github.io/) | [**ü§ó Dataset**](https://huggingface.co/datasets/Saint-lsy/EndoBench) | [**üìñ arXiv**](https://arxiv.org/abs/2412.01605)

[Jie Liu]()<sup>1*</sup> [Wenxuan Wang]()<sup>1*</sup> [Zizhan Ma]()<sup>2*</sup> [Guolin Huang]()<sup>1</sup> [Yihang Su]()<sup>3</sup> [Kao-Jung Chang]()<sup>4</sup> [Wenting Chen]()<sup>1‚úâ</sup>[Haoliang Li]()<sup>1‚úâ</sup>[Linlin Shen]()<sup>3‚úâ</sup>[Michael Lyu]()<sup>2‚úâ</sup>

 <sup>1</sup>City University of Hong Kong &emsp;<sup>2</sup>Chinese University of Hong Kong&emsp; <sup>3</sup>Shenzhen University &emsp; 

<sup>4</sup>National Yang Ming Chiao Tung University &emsp; <sup>5</sup>Taipei Veterans General Hospital &emsp;



This repository is the official implementation of the paper **MedChain: Bridging the Gap Between LLM Agents and Clinical Practice through Interactive Sequential Benchmarking.**

## üöÄOverview

In this paper, we introduce **MedChain**, a novel benchmark designed to bridge the gap between Large Language Model (LLM) agents and real-world clinical decision-making (CDM).  Unlike existing medical benchmarks that focus on isolated tasks, MedChain emphasizes three core features of clinical practice: **personalization**, **interactivity**, and **sequentiality**.

**MedChain** comprises **12,163** rigorously validated clinical cases spanning **19 medical specialties** and **156 sub-categories**, including **7,338 medical images** with reports.  Each case progresses through five sequential stages:
1Ô∏è‚É£ **Specialty Referral**
2Ô∏è‚É£ **History-taking**
3Ô∏è‚É£ **Examination**
4Ô∏è‚É£ **Diagnosis**
5Ô∏è‚É£ **Treatment**

To address the challenges of MedChain, the authors propose **MedChain-Agent**, a multi-agent framework integrating:
- **Three specialized agents** (General, Summarizing, Feedback) for collaborative decision-making.
- **MedCase-RAG**, a retrieval-augmented module that dynamically expands a structured medical case database (12D feature vectors) for context-aware reasoning.

**Key Results**:

- MedChain-Agent outperforms state-of-the-art models (e.g., GPT-4o, Claude-3.5) with an **average score of 0.5269** across tasks, showcasing superior adaptability in sequential CDM.
- Ablation studies confirm the critical roles of **feedback mechanisms** and **MedCase-RAG** .
- The benchmark exposes limitations in existing LLMs, with single-agent models scoring ‚â§0.4327 due to error propagation in sequential stages.

**MedChain** sets a new standard for evaluating AI in clinical workflows, highlighting the need for frameworks that mirror real-world complexity while enabling reliable, patient-centric decision-making.  The dataset and code will be released publicly to foster progress in medical AI.

![overview](assets/figure1-1.jpg)

<img src="assets/figure1-2.jpg" alt="overview" style="zoom:35%;" />

<img src="assets/figure1-3.jpg" alt="overview" style="zoom:35%;" />

## üì¶Code

1. You can find the workflow code for the five core tasks of MedChain in the `task_framework`, and they can be individually tested.   

   ```python
   cd task_framework
   Specialty ReferralÔºö  python task1_triage.py
   History-takingÔºö      python task2_interrogation.py
   ExaminationÔºö         python task3_image.py
   DiagnosisÔºö           python task4_diagnosis.py
   TreatmentÔºö           python task5_treatment.py
   ```

2. You can locate the core workflow code for **MedChain-Agent**, **MedCase-RAG**, and the feedback mechanism in the `main.py` file, and run tests to evaluate MedChain-Agent.

   ```python
   python main.py
   ```

3. Below are the comparative and ablation study results for MedChain-Agent.

![comparison](assets/exp1.jpg)

![comparison](assets/exp2.jpg)

For More Details, please see our paper.

## üîç Insights
1. **Sequential clinical decision-making exposes critical gaps in current AI systems**, with single-agent models achieving only 43.27% average accuracy (Claude-3.5), while MedChain-Agent improves performance to 52.69% through multi-agent collaboration and error mitigation.  
2. **Structured medical knowledge retrieval (MedCase-RAG) drives significant performance gains**, contributing a **8.11% improvement** in clinical task accuracy by enabling dynamic case matching through 12-dimensional feature vectors.  
3. **Iterative feedback mechanisms are pivotal for clinical reasoning**, reducing error propagation and boosting average scores by **3.73%** through continuous refinement across sequential stages.  

## üéàAcknowledgements
Weare particularly indebted to the administrators of the iiyi website for their generosity in allowing us to utilize their data for our research purposes. We would like to acknowledge the assistance provided by Claude-3.5 in proofreading our manuscript for grammatical accuracy and in facilitating the creation of LaTeX tables.
## üìúCitation
If you find this work helpful for your project,please consider citing our  paper.
